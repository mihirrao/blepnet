{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BLEPNet",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N9P2cb2JzRw"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "Ovp69UjYJ1Km"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "from tqdm.notebook import tqdm as tqdm\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import itertools\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import scipy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install scikit-plot\n",
        "import scikitplot as skplt\n",
        "import sklearn\n",
        "from sklearn.model_selection import KFold\n",
        "from collections import Counter\n",
        "import requests\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sdAxUu2J93v"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yaJyqyTJs_n"
      },
      "source": [
        "Load compiled dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbfoRUuOJqEI"
      },
      "source": [
        "epitope_dataset = pd.read_csv('/content/Drive/MyDrive/epitope_dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KdTm0zMMT9s"
      },
      "source": [
        "antigen_dataset = pd.read_csv('/content/Drive/MyDrive/antigen_dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weIoEHXJR_5k"
      },
      "source": [
        "Quantitative Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOoc98YAfskq"
      },
      "source": [
        "def box_plot(data, label, title):\r\n",
        "    data = list(data)\r\n",
        "    data.sort()\r\n",
        "    mean=np.mean(data)\r\n",
        "    median=np.median(data)\r\n",
        "    mode=stats.mode(data)[0][0]\r\n",
        "    print(stats.mode(data))\r\n",
        "    sem = stats.sem(data)\r\n",
        "    plt.figure(figsize=(6,3))\r\n",
        "    #plt.axvline(x=mode, color='b', linestyle='--', label=\"Mode = \" + str(mode))\r\n",
        "    plt.axvline(x=mean+2*sem, color='b', linestyle='--')\r\n",
        "    plt.axvline(x=mean-2*sem, color='b', linestyle='--', label=\"Mean +/- 2*SEM = \" + str(round(mean,2)) + \" +/- \" + str(round(2*sem, 2)))\r\n",
        "    plt.legend()\r\n",
        "    plt.yticks(rotation=90)\r\n",
        "    plt.title(title)\r\n",
        "    plt.boxplot(data, vert=False, showfliers=False, showmeans=True, labels=[\"\"], widths=0.4, notch=True)\r\n",
        "    plt.xlabel(label)\r\n",
        "    plt.show()\r\n",
        "    print(\"Median\", median)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICdbCupMjHxN"
      },
      "source": [
        "Average Antigen Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT_D-XHZSV7L"
      },
      "source": [
        "#Average Antigen Length\r\n",
        "print(\"Average Antigen Length: \" + str(antigen_dataset['Antigen Length'].mean()) + \" +/- \"  + str(stats.sem(antigen_dataset['Antigen Length'])))\r\n",
        "box_plot(sorted(antigen_dataset['Antigen Length']), \"Antigen Length (amino acids)\", \"All antigen lengths\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h29g9rjbjJHp"
      },
      "source": [
        "Average number of epitopes per antigen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3vEPsvgSrs7"
      },
      "source": [
        "#Average number of epitopes per antigen\r\n",
        "counts = []\r\n",
        "for row in range(antigen_dataset.shape[0]):\r\n",
        "    counts.append(len(literal_eval(antigen_dataset.at[row, 'Epitope Starts'])))\r\n",
        "print(\"Average number of epitopes per antigen: \" + str(sum(counts)/len(counts)) + \" +/- \" + str(stats.sem(counts)))\r\n",
        "box_plot(sorted(counts), \"Number of Epitopes\", \"Number of epitopes in antigens\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbyVhIbxjKuQ"
      },
      "source": [
        "Average length of epitope"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYZqRSP3Tc5X"
      },
      "source": [
        "#Average length of epitope\r\n",
        "print(\"Average epitope length: \" + str(epitope_dataset['Epitope Length'].mean()) + \" +/- \" + str(stats.sem(epitope_dataset['Epitope Length'])))\r\n",
        "box_plot(sorted(epitope_dataset['Epitope Length']), \"Epitope Length (amino acids)\", \"Length of epitopes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq4X3x3yjMNe"
      },
      "source": [
        "Average distance from start of sequence to start of first epitope"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "collapsed": true,
        "id": "WOtLPM38Tc4T"
      },
      "source": [
        "#Average distance from start of sequence to start of first epitope\n",
        "start_distances = []\n",
        "for row in range(antigen_dataset.shape[0]):\n",
        "    start_distances.append(min(literal_eval(antigen_dataset.at[row, 'Epitope Starts'])))\n",
        "\n",
        "print(\"Average distance of first epitope from start of sequence: \" + str(sum(start_distances)/len(start_distances)) + \" +/- \" + str(stats.sem(start_distances)))\n",
        "\n",
        "box_plot(sorted(start_distances), \"Start Distances (amino acids)\", \"Distance to first epitope from sequence start\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTFnJwJijNug"
      },
      "source": [
        "Average distance from end of sequence to end of last epitope"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJCTQx6soOdW"
      },
      "source": [
        "#Average distance from end of sequence to end of last epitope\r\n",
        "end_distances = []\r\n",
        "for row in range(antigen_dataset.shape[0]):\r\n",
        "    end_distances.append(len(antigen_dataset.at[row, 'Antigen Sequence']) - max(literal_eval(antigen_dataset.at[row, 'Epitope Ends'])) + 1)\r\n",
        "\r\n",
        "print(\"Average distance of last epitope from end of sequence: \" + str(sum(end_distances)/len(end_distances)) + \" +/- \" + str(stats.sem(end_distances)))\r\n",
        "\r\n",
        "box_plot(sorted(end_distances), \"End Distances (amino acids)\", \"Distance to end of sequence after last epitope\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDMEGyHajPdV"
      },
      "source": [
        "average segment length of overall immunostimulant region"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31CT0xqBjKFw"
      },
      "source": [
        "#average segment length of overall immunostimulant region\r\n",
        "\r\n",
        "lengths = []\r\n",
        "\r\n",
        "for row in tqdm(range(antigen_dataset.shape[0])):\r\n",
        "    sequence = antigen_dataset.at[row, 'Antigen Sequence']\r\n",
        "\r\n",
        "    translation = [0 for i in range(len(sequence))]\r\n",
        "    \r\n",
        "    for i in range(len(literal_eval(antigen_dataset.at[row, \"Epitope Starts\"]))):\r\n",
        "        for x in range(literal_eval(antigen_dataset.at[row, \"Epitope Starts\"])[i]-1, literal_eval(antigen_dataset.at[row, \"Epitope Ends\"])[i]):\r\n",
        "            translation[x] = 1\r\n",
        "    \r\n",
        "    groups = []\r\n",
        "    temp = []\r\n",
        "    count = 0\r\n",
        "    for i in translation:\r\n",
        "        count += 1\r\n",
        "        if len(temp) == 0:\r\n",
        "            temp.append(i)\r\n",
        "        else:\r\n",
        "            if temp[-1] == i:\r\n",
        "                temp.append(i)\r\n",
        "            else:\r\n",
        "                groups.append(temp)\r\n",
        "                temp = []\r\n",
        "                temp.append(i)\r\n",
        "        if count == len(translation):\r\n",
        "            groups.append(temp)\r\n",
        "\r\n",
        "    for group in groups:\r\n",
        "        if group[0] == 1:\r\n",
        "            lengths.append(len(group))   \r\n",
        "\r\n",
        "print(\"Average length of immunostimulant region, regardless of the number of unique epitopes that compose it: \" + str(sum(lengths)/len(lengths)) + \" +/- \" + str(stats.sem(lengths)))\r\n",
        "box_plot(sorted(lengths), \"Immunostimulant Region Length (amino acids)\", \"Length of immunostimulant region\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzrZZRwujRTC"
      },
      "source": [
        "number of immunostimulant regions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz7xyW5NuQ4H",
        "collapsed": true
      },
      "source": [
        "#number of immunostimulant regions\r\n",
        "\r\n",
        "counts = []\r\n",
        "\r\n",
        "for row in tqdm(range(antigen_dataset.shape[0])):\r\n",
        "    sequence = antigen_dataset.at[row, 'Antigen Sequence']\r\n",
        "\r\n",
        "    translation = [0 for i in range(len(sequence))]\r\n",
        "    \r\n",
        "    for i in range(len(literal_eval(antigen_dataset.at[row, \"Epitope Starts\"]))):\r\n",
        "        for x in range(literal_eval(antigen_dataset.at[row, \"Epitope Starts\"])[i]-1, literal_eval(antigen_dataset.at[row, \"Epitope Ends\"])[i]):\r\n",
        "            translation[x] = 1\r\n",
        "    \r\n",
        "    groups = []\r\n",
        "    temp = []\r\n",
        "    count = 0\r\n",
        "    for i in translation:\r\n",
        "        count += 1\r\n",
        "        if len(temp) == 0:\r\n",
        "            temp.append(i)\r\n",
        "        else:\r\n",
        "            if temp[-1] == i:\r\n",
        "                temp.append(i)\r\n",
        "            else:\r\n",
        "                groups.append(temp)\r\n",
        "                temp = []\r\n",
        "                temp.append(i)\r\n",
        "        if count == len(translation):\r\n",
        "            groups.append(temp)\r\n",
        "\r\n",
        "    counts.append(len([i for i in groups if i[0] == 1]))  \r\n",
        "\r\n",
        "print(\"Average number of immunostimulant region, regardless of the number of unique epitopes that compose it: \" + str(sum(counts)/len(counts)) + \" +/- \" + str(stats.sem(counts)))\r\n",
        "box_plot(sorted(counts), \"Immunostimulant Region Count\", \"Number of immunostimulant regions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhZapsSljT4t"
      },
      "source": [
        "kmer frequency analysis for 3 <= k <= 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7JGynNI-GsZ"
      },
      "source": [
        "#kmer frequency analysis for 3 <= k <= 10\r\n",
        "#difference = %epitope area taken - %non-epitope area taken\r\n",
        "\r\n",
        "ks = [i for i in range(3,11)]\r\n",
        "\r\n",
        "def kmers(sequence, ks):\r\n",
        "    previous = []\r\n",
        "    for i in range(len(sequence)):\r\n",
        "        for k in ks:\r\n",
        "            mer = \"\"\r\n",
        "            try:\r\n",
        "                mer = sequence[i:i+k]\r\n",
        "            except: #index out of bounds\r\n",
        "                mer = sequence[i:]\r\n",
        "            if len(mer) == k and mer not in previous:\r\n",
        "                previous.append(mer)\r\n",
        "                yield mer\r\n",
        "\r\n",
        "ep_counts = {}\r\n",
        "non_ep_counts = {}\r\n",
        "\r\n",
        "for row in tqdm(range(antigen_dataset.shape[0])):\r\n",
        "    sequence = antigen_dataset.at[row, 'Antigen Sequence']\r\n",
        "\r\n",
        "    translation = [0 for i in range(len(sequence))]\r\n",
        "    \r\n",
        "    for i in range(len(literal_eval(antigen_dataset.at[row, \"Epitope Starts\"]))):\r\n",
        "        for x in range(literal_eval(antigen_dataset.at[row, \"Epitope Starts\"])[i]-1, literal_eval(antigen_dataset.at[row, \"Epitope Ends\"])[i]):\r\n",
        "            translation[x] = 1\r\n",
        "\r\n",
        "    ep_indices = [i for i,j in enumerate(translation) if j == 1]\r\n",
        "    non_ep_indices = [i for i,j in enumerate(translation) if j == 0]\r\n",
        "\r\n",
        "    ep_regions = []\r\n",
        "    non_ep_regions = []\r\n",
        "\r\n",
        "    for i in range(len(ep_indices)):\r\n",
        "        try:\r\n",
        "            if ep_indices[i] + 1 != ep_indices[i+1]:\r\n",
        "                ep_regions.append(i+1)\r\n",
        "        except:\r\n",
        "            #reached max index\r\n",
        "            if ep_indices[i] - 1 != ep_indices[i-1]:\r\n",
        "                ep_regions.append(i+1)\r\n",
        "    \r\n",
        "    for i in range(len(non_ep_indices)):\r\n",
        "        try:\r\n",
        "            if non_ep_indices[i] + 1 != non_ep_indices[i+1]:\r\n",
        "                non_ep_regions.append(i+1)\r\n",
        "        except:\r\n",
        "            #reached max index\r\n",
        "            if non_ep_indices[i] - 1 != non_ep_indices[i-1]:\r\n",
        "                non_ep_regions.append(i+1)\r\n",
        "    \r\n",
        "    for i in range(len(ep_regions)):\r\n",
        "        sub = \"\"\r\n",
        "        if i == 0 and ep_regions[i] != len(sequence):\r\n",
        "            sub = sequence[:ep_regions[i]]\r\n",
        "        elif ep_regions[i] == len(sequence):\r\n",
        "            sub = sequence[ep_regions[i-1] - 1:]\r\n",
        "        else:\r\n",
        "            sub = sequence[ep_regions[i-1] - 1 : ep_regions[i]]\r\n",
        "\r\n",
        "        for mer in kmers(sub,ks):\r\n",
        "            if mer not in ep_counts:\r\n",
        "                ep_counts[mer] = []\r\n",
        "            ep_counts[mer].append(sub.count(mer)*len(mer)/translation.count(1))\r\n",
        "    \r\n",
        "    for i in range(len(non_ep_regions)):\r\n",
        "        sub = \"\"\r\n",
        "        if i == 0 and non_ep_regions[i] != len(sequence):\r\n",
        "            sub = sequence[:non_ep_regions[i]]\r\n",
        "        elif non_ep_regions[i] == len(sequence):\r\n",
        "            sub = sequence[non_ep_regions[i-1] - 1:]\r\n",
        "        else:\r\n",
        "            sub = sequence[non_ep_regions[i-1] - 1 : non_ep_regions[i]]\r\n",
        "\r\n",
        "        for mer in kmers(sub,ks):\r\n",
        "            if mer not in non_ep_counts:\r\n",
        "                non_ep_counts[mer] = []\r\n",
        "            non_ep_counts[mer].append(sub.count(mer)*len(mer)/translation.count(0))\r\n",
        "\r\n",
        "ep_counts = {k:sum(v)/antigen_dataset.shape[0] for k,v in ep_counts.items()}\r\n",
        "non_ep_counts = {k:sum(v)/antigen_dataset.shape[0] for k,v in non_ep_counts.items()}\r\n",
        "\r\n",
        "diff = {key: ep_counts[key] - non_ep_counts.get(key, 0) for key in ep_counts.keys()} \r\n",
        "diff = dict(sorted(diff.items(), key=lambda item: item[1], reverse=True))\r\n",
        "print(diff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8z4ZS96ohve"
      },
      "source": [
        "Preprocessing and batch training setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pujPE-ZPqbHs"
      },
      "source": [
        "def pad(full, k):\r\n",
        "    pad_req = k-len(full)\r\n",
        "    left_pad = round(pad_req/2)\r\n",
        "    right_pad = pad_req-left_pad\r\n",
        "\r\n",
        "    for i in range(left_pad):\r\n",
        "        full = \"0\" + full\r\n",
        "    for i in range(right_pad):\r\n",
        "        full += \"0\"\r\n",
        "\r\n",
        "    return full\r\n",
        "\r\n",
        "def gen_kmers(row, ids, kmers, classes, thresh=1):\r\n",
        "\r\n",
        "    sequence = row['Antigen Sequence']\r\n",
        "    ep_starts = literal_eval(row['Epitope Starts'])\r\n",
        "    ep_ends = literal_eval(row['Epitope Ends'])\r\n",
        "    \r\n",
        "    if len(sequence) < k:\r\n",
        "        sequence = pad(sequence, k)\r\n",
        "\r\n",
        "    translation = [0 for i in range(len(sequence))]\r\n",
        "\r\n",
        "    for i in range(len(ep_starts)):\r\n",
        "        for x in range(ep_starts[i]-1, ep_ends[i]):\r\n",
        "            translation[x] = 1\r\n",
        "\r\n",
        "    for i in range(0,len(translation)-k+1):\r\n",
        "        ids.append(row['Antigen UniProt'])\r\n",
        "        try:\r\n",
        "            kmers.append(sequence[i:i+k])\r\n",
        "        except:\r\n",
        "            kmers.append(sequence[i:])\r\n",
        "\r\n",
        "        mer = \"\"\r\n",
        "        try:\r\n",
        "            mer = translation[i:i+k]\r\n",
        "        except:\r\n",
        "            mer = translation[i:]\r\n",
        "        \r\n",
        "        found_pos = mer.count(1)\r\n",
        "        num_eps = 0\r\n",
        "        for x in range(len(ep_starts)):\r\n",
        "            ep_start = ep_starts[x] - 1\r\n",
        "            ep_end = ep_ends[x] - 1\r\n",
        "\r\n",
        "            if i >= ep_start and i <= ep_end:\r\n",
        "                num_eps += 1\r\n",
        "        if (found_pos/k >= thresh):\r\n",
        "            classes.append(1)\r\n",
        "        else:\r\n",
        "            classes.append(0)\r\n",
        "\r\n",
        "\r\n",
        "def digitize(seq, key):\r\n",
        "    return [key[i] for i in seq]\r\n",
        "\r\n",
        "key = {'A':1,\r\n",
        "'B':0,\r\n",
        "'C':2,\r\n",
        "'D':3,\r\n",
        "'E':4,\r\n",
        "'F':5,\r\n",
        "'G':6,\r\n",
        "'H':7,\r\n",
        "'I':8,\r\n",
        "'J':0,\r\n",
        "'K':9,\r\n",
        "'L':10,\r\n",
        "'M':11,\r\n",
        "'N':12,\r\n",
        "'O':0,\r\n",
        "'P':13,\r\n",
        "'Q':14,\r\n",
        "'R':15,\r\n",
        "'S':16,\r\n",
        "'T':17,\r\n",
        "'U':0,\r\n",
        "'V':18,\r\n",
        "'W':19,\r\n",
        "'X':0,\r\n",
        "'Y':20,\r\n",
        "'Z':0,\r\n",
        "'0':0}\r\n",
        "\r\n",
        "def pre_processing(kmer):\r\n",
        "    return digitize(kmer, key)\r\n",
        "    #return tf.keras.utils.to_categorical(digitize(kmer, key), num_classes=21)\r\n",
        "\r\n",
        "def generator(samples, batch_size=1000,shuffle_data=True):\r\n",
        "    num_samples = len(samples)\r\n",
        "    while True: # Loop forever so the generator never terminates\r\n",
        "        if shuffle_data:\r\n",
        "            np.random.shuffle(samples)\r\n",
        "\r\n",
        "        # Get index to start each batch: [0, batch_size, 2*batch_size, ..., max multiple of batch_size <= num_samples]\r\n",
        "        for offset in range(0, num_samples, batch_size):\r\n",
        "            # Get the samples you'll use in this batch\r\n",
        "            batch_samples = samples[offset:offset+batch_size]\r\n",
        "\r\n",
        "            # Initialise X_train and y_train arrays for this batch\r\n",
        "            X_train = []\r\n",
        "            y_train = []\r\n",
        "\r\n",
        "            # For each example\r\n",
        "            for batch_sample in batch_samples:\r\n",
        "                # Load image (X) and label (y)\r\n",
        "                kmer = batch_sample[0]\r\n",
        "                label = batch_sample[1]\r\n",
        "\r\n",
        "                #apply preprocessing\r\n",
        "\r\n",
        "                kmer = pre_processing(kmer)\r\n",
        "\r\n",
        "                # Add example to arrays\r\n",
        "                X_train.append(kmer)\r\n",
        "                \r\n",
        "                output = [0,0]\r\n",
        "                output[label] = 1\r\n",
        "                y_train.append(output)\r\n",
        "\r\n",
        "            # Make sure they're numpy arrays (as opposed to lists)\r\n",
        "            X_train = np.array(X_train)\r\n",
        "            y_train = np.array(y_train)\r\n",
        "\r\n",
        "            if offset==0:\r\n",
        "                pass\r\n",
        "            \r\n",
        "            yield X_train, y_train\r\n",
        "\r\n",
        "def softmax_binarize(preds):\r\n",
        "    bin = []\r\n",
        "    for pred in preds:\r\n",
        "        bin.append(np.argmax(pred))\r\n",
        "    return bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7ex9A85_qZ_"
      },
      "source": [
        "10-fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "328434SO_qtm"
      },
      "source": [
        "ks = [5,8,9,10,11,12,15,20]\r\n",
        "\r\n",
        "for k_index in tqdm(range(len(ks))):\r\n",
        "    k = ks[k_index]\r\n",
        "\r\n",
        "    cv = 10\r\n",
        "    kf = KFold(n_splits=cv, shuffle=True) # Define the split - into 10 folds\r\n",
        "\r\n",
        "    TRAIN_SETS = []\r\n",
        "    TEST_SETS = []\r\n",
        "\r\n",
        "    for i,j in kf.split(antigen_dataset):\r\n",
        "        TRAIN_SETS.append(antigen_dataset)\r\n",
        "        TEST_SETS.append(antigen_dataset.iloc[j])\r\n",
        "\r\n",
        "    for fold in range(cv):\r\n",
        "        train_fold = TRAIN_SETS[fold]\r\n",
        "        test_fold = TEST_SETS[fold]\r\n",
        "\r\n",
        "        ids = []\r\n",
        "        kmers = []\r\n",
        "        classes = []\r\n",
        "\r\n",
        "        for index in range(train_fold.shape[0]):\r\n",
        "            gen_kmers(train_fold.iloc[index], ids, kmers, classes)\r\n",
        "\r\n",
        "        kmer_df = pd.DataFrame()\r\n",
        "        kmer_df['ID'] = ids\r\n",
        "        kmer_df['kmer'] = kmers\r\n",
        "        kmer_df['class'] = classes\r\n",
        "        kmer_df = kmer_df.drop_duplicates(subset=\"kmer\", keep=\"first\")\r\n",
        "        \r\n",
        "        pos_df = kmer_df[kmer_df['class'] == 1]\r\n",
        "        neg_df = kmer_df[kmer_df['class'] == 0]\r\n",
        "\r\n",
        "        ratio = pos_df.size/neg_df.size\r\n",
        "        neg_df = neg_df.sample(frac=ratio)\r\n",
        "        frames = [pos_df,neg_df]\r\n",
        "        concat_df = pd.concat(frames)\r\n",
        "\r\n",
        "        concat_df.sample(frac=1)\r\n",
        "        train_array = concat_df[['kmer', 'class']].values\r\n",
        "\r\n",
        "        test_ids = []\r\n",
        "        test_kmers = []\r\n",
        "        test_classes = []\r\n",
        "\r\n",
        "        for index in range(test_fold.shape[0]):\r\n",
        "            gen_kmers(test_fold.iloc[index], test_ids, test_kmers, test_classes)\r\n",
        "        \r\n",
        "        og_test_kmer_df = pd.DataFrame()\r\n",
        "        og_test_kmer_df['ID'] = test_ids\r\n",
        "        og_test_kmer_df['kmer'] = test_kmers\r\n",
        "        og_test_kmer_df['class'] = test_classes\r\n",
        "        test_kmer_df = og_test_kmer_df.drop_duplicates(subset=\"kmer\", keep=\"first\")\r\n",
        "        \r\n",
        "        test_pos_df = test_kmer_df[test_kmer_df['class'] == 1]\r\n",
        "        test_neg_df = test_kmer_df[test_kmer_df['class'] == 0]\r\n",
        "\r\n",
        "        test_ratio = test_pos_df.size/test_neg_df.size\r\n",
        "        test_neg_df = test_neg_df.sample(frac=test_ratio)\r\n",
        "        test_frames = [test_pos_df,test_neg_df]\r\n",
        "        test_concat_df = pd.concat(test_frames)\r\n",
        "\r\n",
        "        test_concat_df.sample(frac=1)\r\n",
        "        val_array = test_concat_df[['kmer', 'class']].values\r\n",
        "        \r\n",
        "\r\n",
        "        batch_size=1000       \r\n",
        "        val_batch_size = 1000\r\n",
        "\r\n",
        "        print(train_array.shape, val_array.shape)\r\n",
        "\r\n",
        "        train_gen = generator(train_array, batch_size = batch_size, shuffle_data=True)\r\n",
        "        val_gen = generator(val_array, batch_size=val_batch_size, shuffle_data=True)\r\n",
        "\r\n",
        "        hidden_size = 128\r\n",
        "        model = Sequential()\r\n",
        "        model.add(Embedding(21, 21, input_length=k))\r\n",
        "        model.add(Bidirectional(LSTM(hidden_size, return_sequences=True), merge_mode=\"concat\"))\r\n",
        "        model.add(Bidirectional(LSTM(hidden_size, return_sequences=False), merge_mode=\"concat\"))\r\n",
        "        model.add(Dense(10))\r\n",
        "        model.add(Dense(2, activation='softmax'))\r\n",
        "        model.summary()\r\n",
        "\r\n",
        "        model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=tf.optimizers.Adam(learning_rate=0.01, amsgrad=True))\r\n",
        "        callback = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=10, restore_best_weights=True, verbose=1, baseline=0.1)]\r\n",
        "        history = model.fit(train_gen, epochs=50, verbose=1, validation_data=val_gen,steps_per_epoch=len(train_array)//batch_size, validation_steps=len(val_array)//val_batch_size, validation_freq=1, callbacks=callback)\r\n",
        "        #plt.plot(history.history['accuracy'], label='accuracy')\r\n",
        "        #plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\r\n",
        "        #plt.xlabel('Epoch')\r\n",
        "        #plt.ylabel('Accuracy')\r\n",
        "        #plt.ylim([0.0, 1])\r\n",
        "        #plt.legend(loc='lower right')\r\n",
        "        #plt.show()\r\n",
        "\r\n",
        "        model.save(\"model_\" + str(fold) + \".h5\")\r\n",
        "\r\n",
        "        #classifier performance\r\n",
        "        thresholds = [i*0.05 for i in range(0,21)]\r\n",
        "\r\n",
        "        for fold in range(cv):\r\n",
        "            model = load_model(\"model_\" + str(fold) + \".h5\")\r\n",
        "\r\n",
        "            test_fold = TEST_SETS[fold]\r\n",
        "\r\n",
        "            test_ids = []\r\n",
        "            test_kmers = []\r\n",
        "            test_classes = []\r\n",
        "\r\n",
        "            for index in range(test_fold.shape[0]):\r\n",
        "                gen_kmers(test_fold.iloc[index], test_ids, test_kmers, test_classes)\r\n",
        "            \r\n",
        "            og_test_kmer_df = pd.DataFrame()\r\n",
        "            og_test_kmer_df['ID'] = test_ids\r\n",
        "            og_test_kmer_df['kmer'] = test_kmers\r\n",
        "            og_test_kmer_df['class'] = test_classes\r\n",
        "            test_kmer_df = og_test_kmer_df.drop_duplicates(subset=\"kmer\", keep=\"first\")\r\n",
        "            \r\n",
        "            test_pos_df = test_kmer_df[test_kmer_df['class'] == 1]\r\n",
        "            test_neg_df = test_kmer_df[test_kmer_df['class'] == 0]\r\n",
        "\r\n",
        "            test_ratio = test_pos_df.size/test_neg_df.size\r\n",
        "            test_neg_df = test_neg_df.sample(frac=test_ratio)\r\n",
        "            test_frames = [test_pos_df,test_neg_df]\r\n",
        "            test_concat_df = pd.concat(test_frames)\r\n",
        "\r\n",
        "            test_concat_df.sample(frac=1)\r\n",
        "            val_array = test_concat_df[['kmer', 'class']].values\r\n",
        "\r\n",
        "            batch_size=1000    \r\n",
        "            val_batch_size = 1000\r\n",
        "\r\n",
        "            test_gen = generator(val_array, batch_size=val_batch_size, shuffle_data=False)\r\n",
        "\r\n",
        "            preds = model.predict(test_gen, verbose=1, batch_size=batch_size, steps=len(val_array)//val_batch_size)\r\n",
        "            print(len(preds), len(test_ids))\r\n",
        "            bins = softmax_binarize(preds)\r\n",
        "\r\n",
        "            bins = np.array(bins)\r\n",
        "            truth = val_array[:,1][:len(bins)].astype(int)\r\n",
        "\r\n",
        "            skplt.metrics.plot_confusion_matrix(truth, bins, normalize=True)\r\n",
        "            skplt.metrics.plot_roc_curve(truth, preds)\r\n",
        "            plt.show()\r\n",
        "\r\n",
        "            skplt.metrics.plot_precision_recall_curve(truth, preds)\r\n",
        "            plt.show()\r\n",
        "\r\n",
        "            print(sklearn.metrics.classification_report(truth,bins))\r\n",
        "            \r\n",
        "            #full system performance\r\n",
        "\r\n",
        "            tprs = {}\r\n",
        "            fprs = {}\r\n",
        "\r\n",
        "            tprs_rank = {}\r\n",
        "            fprs_rank = {}\r\n",
        "        \r\n",
        "            for i in thresholds:\r\n",
        "                tprs[i] = []\r\n",
        "                fprs[i] = []\r\n",
        "                tprs_rank[i] = []\r\n",
        "                fprs_rank[i] = []\r\n",
        "\r\n",
        "            fs_ids = og_test_kmer_df['ID'].tolist()\r\n",
        "            fs_val_array = og_test_kmer_df[['kmer', 'class']].values\r\n",
        "\r\n",
        "            fs_val_batch_size = 1000\r\n",
        "            fs_test_gen = generator(fs_val_array, batch_size=fs_val_batch_size, shuffle_data=False)\r\n",
        "\r\n",
        "            predictions = model.predict(fs_test_gen, verbose=1, batch_size=batch_size, steps=len(fs_val_array)//val_batch_size)\r\n",
        "\r\n",
        "            print(len(fs_ids), len(predictions))\r\n",
        "\r\n",
        "            predictions = list(predictions)\r\n",
        "            for i in range(len(fs_ids) - len(predictions)):\r\n",
        "                predictions.append([])\r\n",
        "            og_test_kmer_df['predictions'] = predictions\r\n",
        "\r\n",
        "            for id_index in tqdm(range(len(og_test_kmer_df['ID'].unique()))):\r\n",
        "                id = og_test_kmer_df['ID'].unique()[id_index]\r\n",
        "                if [] not in og_test_kmer_df[og_test_kmer_df['ID'] == id]['predictions'].tolist():\r\n",
        "                    kmer_preds = og_test_kmer_df[og_test_kmer_df['ID'] == id]['predictions'].tolist()\r\n",
        "                    realigned = [[] for i in range(test_fold[test_fold['Antigen UniProt'] == id]['Antigen Length'].tolist()[0])]\r\n",
        "\r\n",
        "                    passed = True\r\n",
        "\r\n",
        "                    if len(kmer_preds) != len(realigned) - k + 1:\r\n",
        "                        print(\"Q issue\")\r\n",
        "                        passed = False\r\n",
        "\r\n",
        "                    if passed:\r\n",
        "                        for x in range(len(kmer_preds)):\r\n",
        "                            for y in range(x,x+k):\r\n",
        "                                realigned[y].append(kmer_preds[x][1])\r\n",
        "\r\n",
        "                        realigned = [sum(i)/len(i) for i in realigned]\r\n",
        "\r\n",
        "                        sequence = test_fold[test_fold['Antigen UniProt'] == id][\"Antigen Sequence\"].tolist()[0]\r\n",
        "\r\n",
        "                        translation = [0 for i in range(len(sequence))]\r\n",
        "                        for i in range(len(literal_eval(test_fold[test_fold['Antigen UniProt'] == id][\"Epitope Starts\"].tolist()[0]))):\r\n",
        "                            for x in range(literal_eval(test_fold[test_fold['Antigen UniProt'] == id][\"Epitope Starts\"].tolist()[0])[i]-1, literal_eval(test_fold[test_fold['Antigen UniProt'] == id][\"Epitope Ends\"].tolist()[0])[i]):\r\n",
        "                                translation[x] = 1\r\n",
        "                        \r\n",
        "                        if translation.count(1) != 0 and translation.count(1) != len(translation): #avoid division by zero for tpr and fpr\r\n",
        "                            sigmoid = lambda x: 1/(1+(np.exp(-10*(x-0.5))))\r\n",
        "                            #limit = lambda x: x if 0<=x and x<=1 else (0 if x<0 else 1)\r\n",
        "                            diff_based = [0 for i in range(len(realigned))]\r\n",
        "                            diff_based_count = [0 for i in range(len(realigned))]\r\n",
        "                            extent = 100\r\n",
        "                            search_diff = dict(list(diff.items())[:extent] + list(diff.items())[-1*extent:])\r\n",
        "                            for target in list(search_diff.keys()):\r\n",
        "                                for start in (m.start() for m in re.finditer('(?='+ target +')', sequence)):\r\n",
        "                                    for ind in range(start,start+len(target)):\r\n",
        "                                        if abs(realigned[ind] - 0.5) <= 0.1:\r\n",
        "                                            diff_based[ind] += search_diff[target]\r\n",
        "                                            diff_based_count[ind] += 1\r\n",
        "                            diff_based = [diff_based[i]/diff_based_count[i] if diff_based_count[i] != 0 else 0 for i in range(len(diff_based))]\r\n",
        "                            diff_based = [sigmoid(((1/(1000*diff_based[i])) + realigned[i])/2) if diff_based[i] != 0 else realigned[i] for i in range(len(realigned))]\r\n",
        "\r\n",
        "                            for thresh in thresholds:\r\n",
        "                                tp = 0\r\n",
        "                                fp = 0\r\n",
        "                                tn = 0\r\n",
        "                                fn = 0\r\n",
        "\r\n",
        "                                where = []\r\n",
        "                                for q in realigned:\r\n",
        "                                    if q >=thresh:\r\n",
        "                                        where.append(1)\r\n",
        "                                    else:\r\n",
        "                                        where.append(0)\r\n",
        "\r\n",
        "                                for index in range(len(translation)):\r\n",
        "                                    if where[index] == 1: # predicted positive\r\n",
        "                                        if translation[index] == 1:\r\n",
        "                                            tp += 1\r\n",
        "                                        else:\r\n",
        "                                            fp += 1\r\n",
        "                                    elif where[index] == 0: # predicted negative\r\n",
        "                                        if translation[index] == 1:\r\n",
        "                                            fn += 1\r\n",
        "                                        else:\r\n",
        "                                            tn += 1\r\n",
        "                                    else:\r\n",
        "                                        print(\"Binarized Prediction Error\")\r\n",
        "                                \r\n",
        "                                tpr = tp/(tp+fn)\r\n",
        "                                fpr = fp/(fp+tn)\r\n",
        "\r\n",
        "                                tprs[thresh].append(tpr)\r\n",
        "                                fprs[thresh].append(fpr)\r\n",
        "\r\n",
        "                                #ranking implementation\r\n",
        "\r\n",
        "                                tp = 0\r\n",
        "                                fp = 0\r\n",
        "                                tn = 0\r\n",
        "                                fn = 0\r\n",
        "\r\n",
        "                                where = []\r\n",
        "                                for q in diff_based:\r\n",
        "                                    if q >=thresh:\r\n",
        "                                        where.append(1)\r\n",
        "                                    else:\r\n",
        "                                        where.append(0)\r\n",
        "\r\n",
        "                                for index in range(len(translation)):\r\n",
        "                                    if where[index] == 1: # predicted positive\r\n",
        "                                        if translation[index] == 1:\r\n",
        "                                            tp += 1\r\n",
        "                                        else:\r\n",
        "                                            fp += 1\r\n",
        "                                    elif where[index] == 0: # predicted negative\r\n",
        "                                        if translation[index] == 1:\r\n",
        "                                            fn += 1\r\n",
        "                                        else:\r\n",
        "                                            tn += 1\r\n",
        "                                    else:\r\n",
        "                                        print(\"Binarized Prediction Error\")\r\n",
        "                                \r\n",
        "                                tpr = tp/(tp+fn)\r\n",
        "                                fpr = fp/(fp+tn)\r\n",
        "\r\n",
        "                                tprs_rank[thresh].append(tpr)\r\n",
        "                                fprs_rank[thresh].append(fpr)\r\n",
        "                            \r\n",
        "\r\n",
        "            tprs = {k:sum(v)/len(v) for k,v in tprs.items()}\r\n",
        "            fprs = {k:sum(v)/len(v) for k,v in fprs.items()}\r\n",
        "\r\n",
        "            if k == 10:\r\n",
        "                print(tprs)\r\n",
        "                print(fprs)\r\n",
        "\r\n",
        "            tprs_rank = {k:sum(v)/len(v) for k,v in tprs_rank.items()}\r\n",
        "            fprs_rank = {k:sum(v)/len(v) for k,v in fprs_rank.items()}\r\n",
        "\r\n",
        "            plt.plot(list(fprs.values()), list(tprs.values()), label=\"AUC=\" + str(round(sklearn.metrics.auc(list(fprs.values()), list(tprs.values())), 3)))\r\n",
        "            plt.plot(list(fprs_rank.values()), list(tprs_rank.values()), label=\"With Ranks AUC=\" + str(round(sklearn.metrics.auc(list(fprs_rank.values()), list(tprs_rank.values())), 3)))\r\n",
        "            plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}